{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import os\n",
    "import matplotlib.image as img\n",
    "import tqdm\n",
    "from vae import Basic_VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = []\n",
    "directory = \"data/cats\"\n",
    "count = 0\n",
    "\n",
    "for catpic in os.listdir(directory):\n",
    "    if count < 1000:\n",
    "        # read from image and convert to tensor\n",
    "        im = torch.tensor(img.imread(os.path.join(directory, catpic))).float()\n",
    "        # permute to (channels, height, width) for conv2d layer\n",
    "        im = torch.permute(im, (2, 0, 1))\n",
    "        # normalize to range between -1 and 1\n",
    "        im = im / 128 - 1\n",
    "        cats.append(im)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "cats = torch.stack(cats)\n",
    "print(cats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test/Training/Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into what we use for testing and not testing\n",
    "training_test_split = 0.75\n",
    "training_test_cutoff = int(cats.shape[0] * training_test_split + 1)\n",
    "random_perm = torch.randperm(cats.shape[0])\n",
    "not_test_tensor = cats[:training_test_cutoff]\n",
    "testing_tensor = cats[training_test_cutoff:]\n",
    "# Split the data into what we use for training and cross validation\n",
    "training_cv_split = 0.8\n",
    "training_cv_cutoff = int(not_test_tensor.shape[0] * training_cv_split)\n",
    "training_tensor = not_test_tensor[:training_cv_cutoff]\n",
    "cv_tensor = not_test_tensor[training_cv_cutoff:]\n",
    "print(training_tensor.shape)\n",
    "print(cv_tensor.shape)\n",
    "print(testing_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Hyperparameters and Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae import Basic_VAE\n",
    "\n",
    "hidden_dims = [16, 32, 64, 128]\n",
    "latent_dim = 64\n",
    "in_dim = 3\n",
    "model = Basic_VAE(in_dim, hidden_dims, latent_dim)\n",
    "# encoder = Encoder(in_dim, hidden_dims, latent_dim) for testing\n",
    "# decoder = Decoder(latent_dim, hidden_dims) for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[[ 0.6768,  0.7690, -0.0362,  ...,  0.1853,  0.1242,  0.0943],\n",
       "           [ 0.0937,  0.6233,  0.8810,  ...,  0.2799,  0.2491, -0.4956],\n",
       "           [ 0.3017,  0.3556,  0.2793,  ...,  0.7202,  0.5164,  0.1278],\n",
       "           ...,\n",
       "           [-0.1128,  0.0221, -0.5704,  ...,  0.8654,  0.4082, -0.1877],\n",
       "           [ 0.0211,  0.2953,  0.4599,  ...,  0.5476, -0.0114,  0.2275],\n",
       "           [ 0.1039,  0.4965,  0.3077,  ..., -0.0439,  0.1748,  0.2031]],\n",
       " \n",
       "          [[-0.0504,  0.6194, -0.5115,  ...,  0.4151,  0.4376,  0.2416],\n",
       "           [-0.7110, -0.9640, -0.8447,  ...,  0.8489, -0.8827,  0.0489],\n",
       "           [-0.8370, -0.8408, -0.6446,  ...,  0.5381,  0.9613,  0.6388],\n",
       "           ...,\n",
       "           [-0.3912,  0.0827, -0.7018,  ...,  0.5258,  0.5986,  0.0882],\n",
       "           [ 0.4367, -0.3559, -0.2975,  ..., -0.1871,  0.7205, -0.1976],\n",
       "           [-0.0463, -0.1130,  0.7096,  ...,  0.2167,  0.2324,  0.4482]],\n",
       " \n",
       "          [[ 0.4742, -0.2278, -0.2529,  ..., -0.2451,  0.0458, -0.3998],\n",
       "           [-0.3289, -0.4558,  0.9152,  ..., -0.0550, -0.5672, -0.7290],\n",
       "           [ 0.3206, -0.5469,  0.0852,  ...,  0.7120,  0.5239, -0.3778],\n",
       "           ...,\n",
       "           [-0.0954, -0.2932, -0.3926,  ...,  0.5555, -0.1137, -0.3885],\n",
       "           [ 0.2876, -0.3376, -0.6895,  ..., -0.2415,  0.3934,  0.3701],\n",
       "           [ 0.1619,  0.7844,  0.9260,  ...,  0.2335,  0.1623,  0.1399]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1755,  0.8656, -0.2998,  ..., -0.0540,  0.1963, -0.2785],\n",
       "           [-0.5974, -0.4823, -0.0983,  ...,  0.5785,  0.3383, -0.7025],\n",
       "           [ 0.2415,  0.4298, -0.9056,  ...,  0.4095,  0.6906, -0.3085],\n",
       "           ...,\n",
       "           [ 0.1435,  0.8660, -0.3563,  ...,  0.0070,  0.1333,  0.1225],\n",
       "           [ 0.0465, -0.5146, -0.0269,  ...,  0.2263,  0.5452, -0.3456],\n",
       "           [-0.1789,  0.4497, -0.1963,  ...,  0.0999,  0.2646, -0.2743]],\n",
       " \n",
       "          [[ 0.7843,  0.3674, -0.4545,  ..., -0.7550,  0.2716,  0.4861],\n",
       "           [ 0.0984, -0.5621, -0.0946,  ..., -0.9365, -0.8395, -0.1229],\n",
       "           [ 0.2371, -0.9205, -0.0218,  ...,  0.1123,  0.1749,  0.5623],\n",
       "           ...,\n",
       "           [ 0.1732,  0.2442,  0.8710,  ..., -0.4292, -0.2127, -0.1325],\n",
       "           [-0.0688,  0.5139,  0.2473,  ...,  0.1668,  0.2169,  0.3090],\n",
       "           [-0.2862, -0.6118,  0.3883,  ...,  0.0462,  0.4021,  0.4120]],\n",
       " \n",
       "          [[-0.2290,  0.6053,  0.2802,  ..., -0.1008,  0.1852, -0.1075],\n",
       "           [-0.1132, -0.4717,  0.8724,  ...,  0.5851,  0.1260, -0.1125],\n",
       "           [ 0.3550, -0.7995,  0.7500,  ..., -0.1392,  0.7562, -0.1339],\n",
       "           ...,\n",
       "           [ 0.4803,  0.6347, -0.6254,  ..., -0.0374,  0.3952,  0.1167],\n",
       "           [ 0.1847,  0.6269, -0.2139,  ..., -0.5485, -0.1283,  0.4580],\n",
       "           [-0.4632,  0.7371,  0.1756,  ...,  0.1446, -0.0796,  0.1344]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1122,  0.0138,  0.6731,  ..., -0.4345, -0.7394, -0.1730],\n",
       "           [ 0.5103, -0.4853, -0.9080,  ...,  0.4070, -0.6507, -0.8131],\n",
       "           [ 0.2376, -0.4065,  0.0623,  ...,  0.4326, -0.6060,  0.8691],\n",
       "           ...,\n",
       "           [ 0.6669, -0.9747, -0.8221,  ...,  0.3733,  0.6032, -0.7378],\n",
       "           [ 0.1103,  0.2708, -0.8073,  ...,  0.1940,  0.1067,  0.0980],\n",
       "           [-0.0855, -0.2027,  0.3023,  ...,  0.0338,  0.5041,  0.2207]],\n",
       " \n",
       "          [[ 0.5279,  0.8819,  0.4811,  ..., -0.4685,  0.6547, -0.1395],\n",
       "           [-0.6332,  0.6781, -0.3581,  ...,  0.5092, -0.8477,  0.2194],\n",
       "           [-0.2979,  0.7320,  0.5831,  ...,  0.1491,  0.5721, -0.0870],\n",
       "           ...,\n",
       "           [-0.3361,  0.4302,  0.3223,  ...,  0.5454, -0.7040,  0.5515],\n",
       "           [ 0.6376,  0.1053,  0.7853,  ..., -0.8421, -0.5805,  0.1612],\n",
       "           [ 0.6006,  0.2478,  0.7806,  ...,  0.3616, -0.3947,  0.4037]],\n",
       " \n",
       "          [[-0.0035, -0.4420,  0.6637,  ...,  0.2881,  0.1481,  0.7923],\n",
       "           [ 0.5656, -0.6221, -0.9123,  ...,  0.2539, -0.0751, -0.6940],\n",
       "           [ 0.3232,  0.7669, -0.4053,  ...,  0.2679,  0.4092,  0.6845],\n",
       "           ...,\n",
       "           [ 0.3562, -0.9713, -0.4640,  ...,  0.5733,  0.5042, -0.4628],\n",
       "           [ 0.1953,  0.4146, -0.8116,  ...,  0.4983, -0.3598,  0.2391],\n",
       "           [-0.1168, -0.1785, -0.2502,  ..., -0.1359, -0.1962, -0.1067]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-0.3480, -0.4391,  0.1820,  ...,  0.2780,  0.0292, -0.0270],\n",
       "           [-0.6644, -0.4916,  0.3970,  ..., -0.3834,  0.4283,  0.6909],\n",
       "           [-0.2205, -0.3725,  0.6886,  ..., -0.7625, -0.2297, -0.0876],\n",
       "           ...,\n",
       "           [ 0.4628,  0.4134, -0.1816,  ...,  0.0796,  0.0794, -0.1352],\n",
       "           [-0.2350, -0.2910,  0.0677,  ...,  0.0560, -0.4077,  0.7847],\n",
       "           [ 0.2007,  0.1561,  0.3394,  ...,  0.0870, -0.2198, -0.2697]],\n",
       " \n",
       "          [[-0.1056,  0.0791,  0.1147,  ..., -0.0106, -0.1415, -0.0078],\n",
       "           [ 0.2518,  0.1613,  0.5022,  ...,  0.9768,  0.9264, -0.2915],\n",
       "           [ 0.3022, -0.4883,  0.1318,  ...,  0.1542, -0.3405,  0.6800],\n",
       "           ...,\n",
       "           [ 0.5256,  0.8802,  0.5140,  ..., -0.2610, -0.1579,  0.5401],\n",
       "           [ 0.6828,  0.4464, -0.4884,  ...,  0.2405, -0.2117,  0.5944],\n",
       "           [ 0.1956,  0.4404, -0.2572,  ...,  0.6748, -0.1551, -0.1214]],\n",
       " \n",
       "          [[ 0.6547,  0.1430, -0.1352,  ...,  0.6579, -0.0958,  0.2839],\n",
       "           [ 0.0099,  0.7100, -0.7761,  ..., -0.3426,  0.8377,  0.4192],\n",
       "           [-0.2698,  0.9389,  0.5518,  ..., -0.8105,  0.0152,  0.3184],\n",
       "           ...,\n",
       "           [ 0.6938, -0.1254, -0.2084,  ...,  0.7087, -0.6471, -0.4335],\n",
       "           [ 0.6095, -0.4985,  0.4740,  ...,  0.0339,  0.1252,  0.1982],\n",
       "           [-0.1358, -0.1192,  0.1789,  ...,  0.2473, -0.0129, -0.3815]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1742,  0.7049,  0.4134,  ..., -0.5209, -0.3955, -0.0370],\n",
       "           [-0.2546, -0.3651, -0.9718,  ..., -0.2955, -0.5023, -0.5285],\n",
       "           [-0.0720, -0.3625, -0.6715,  ..., -0.3353,  0.1617,  0.2730],\n",
       "           ...,\n",
       "           [ 0.0104, -0.5512, -0.6421,  ..., -0.1194, -0.2558, -0.4527],\n",
       "           [-0.2300,  0.0038,  0.2963,  ...,  0.3963, -0.6396,  0.1071],\n",
       "           [ 0.0725,  0.5150,  0.6902,  ...,  0.4117,  0.4005,  0.1944]],\n",
       " \n",
       "          [[-0.0037,  0.9345,  0.2774,  ...,  0.5274,  0.6214, -0.4983],\n",
       "           [-0.4674, -0.4572,  0.7131,  ...,  0.2905,  0.0630, -0.1002],\n",
       "           [-0.0846,  0.2484,  0.9431,  ...,  0.8210,  0.9211,  0.7003],\n",
       "           ...,\n",
       "           [-0.4051, -0.0904,  0.4225,  ..., -0.4269,  0.0982, -0.5962],\n",
       "           [ 0.3629, -0.5126, -0.3569,  ..., -0.4073, -0.3249,  0.0347],\n",
       "           [ 0.1562, -0.1595,  0.1040,  ..., -0.2166, -0.0746, -0.5134]],\n",
       " \n",
       "          [[ 0.6888,  0.5132,  0.7480,  ..., -0.2056, -0.3691, -0.0281],\n",
       "           [ 0.3492, -0.7571,  0.2060,  ..., -0.3791, -0.8536,  0.0472],\n",
       "           [-0.1356,  0.8753,  0.1737,  ...,  0.4692,  0.2918,  0.1831],\n",
       "           ...,\n",
       "           [-0.1013, -0.4621,  0.3623,  ...,  0.4027,  0.1544,  0.4345],\n",
       "           [ 0.0584, -0.1509, -0.5668,  ...,  0.2671, -0.5133,  0.1499],\n",
       "           [ 0.3639, -0.0526, -0.2404,  ...,  0.1482, -0.2146,  0.0196]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1729, -0.0718,  0.7447,  ..., -0.5967, -0.3345, -0.0135],\n",
       "           [ 0.1343,  0.7527, -0.9450,  ...,  0.4033,  0.1826,  0.0228],\n",
       "           [ 0.0198, -0.0957, -0.9512,  ...,  0.3695,  0.4150,  0.1612],\n",
       "           ...,\n",
       "           [ 0.3057,  0.7758,  0.6845,  ...,  0.4985, -0.6255,  0.1825],\n",
       "           [ 0.3533,  0.4229,  0.2810,  ...,  0.2529, -0.3746,  0.0900],\n",
       "           [-0.0594,  0.1554, -0.0342,  ...,  0.4618,  0.0185, -0.0885]],\n",
       " \n",
       "          [[ 0.2898,  0.5672,  0.3186,  ..., -0.0300,  0.0746,  0.3900],\n",
       "           [-0.5298,  0.8451,  0.9453,  ..., -0.6612, -0.4386,  0.0157],\n",
       "           [-0.1718,  0.1035,  0.0734,  ..., -0.1793,  0.6178,  0.3960],\n",
       "           ...,\n",
       "           [-0.2510, -0.8701,  0.0602,  ...,  0.2226, -0.0823, -0.5138],\n",
       "           [ 0.1442, -0.2664, -0.5787,  ..., -0.4373, -0.0950,  0.6468],\n",
       "           [ 0.6644, -0.0691, -0.5960,  ...,  0.2357, -0.2128, -0.2305]],\n",
       " \n",
       "          [[ 0.0387, -0.0115,  0.0049,  ...,  0.1493, -0.1130, -0.2589],\n",
       "           [ 0.7278,  0.3280, -0.9235,  ...,  0.6705, -0.1584, -0.3894],\n",
       "           [-0.1852, -0.2394,  0.5365,  ...,  0.0892,  0.5863, -0.2716],\n",
       "           ...,\n",
       "           [-0.2120,  0.4957,  0.1918,  ...,  0.2524,  0.3372, -0.1298],\n",
       "           [-0.0815,  0.6478,  0.0693,  ..., -0.0671,  0.4837, -0.3209],\n",
       "           [-0.0528,  0.2675,  0.3330,  ..., -0.2199, -0.1948, -0.2904]]]],\n",
       "        grad_fn=<TanhBackward0>),\n",
       " tensor([[ 0.0420,  0.2177, -0.2484,  ..., -0.1303,  0.2044, -0.3177],\n",
       "         [ 0.3504,  0.0857, -0.0218,  ...,  0.0923,  0.2731, -0.3555],\n",
       "         [ 0.0178, -0.3042,  0.1460,  ..., -0.5989, -0.3536,  0.0534],\n",
       "         ...,\n",
       "         [-0.1692,  0.0603,  0.1994,  ...,  0.0881,  0.3813, -0.4024],\n",
       "         [-0.1865, -0.1278, -0.2910,  ..., -0.2861, -0.4562,  0.0665],\n",
       "         [ 0.5086,  0.1635,  0.5726,  ..., -0.7891,  0.0823, -0.3644]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 2.3040e-02, -7.0906e-02, -2.6940e-01,  ..., -4.1326e-01,\n",
       "           3.3680e-01,  2.9805e-01],\n",
       "         [ 1.9763e-02, -2.3362e-01, -1.5072e-01,  ..., -4.6340e-01,\n",
       "           1.0755e-01,  2.8402e-01],\n",
       "         [-2.0272e-01, -7.4007e-01,  1.0261e+00,  ..., -5.6797e-01,\n",
       "          -1.2284e-01, -5.0638e-01],\n",
       "         ...,\n",
       "         [-2.3076e-02, -1.1038e+00, -7.7786e-02,  ..., -2.4902e-02,\n",
       "          -1.0070e-04, -2.0492e-02],\n",
       "         [ 1.4367e-01, -7.1056e-01,  5.8410e-02,  ...,  3.0533e-01,\n",
       "          -7.5176e-02,  5.0531e-01],\n",
       "         [ 4.2393e-01, -1.1788e+00,  2.3844e-02,  ...,  8.5277e-01,\n",
       "           5.0385e-02,  1.5572e-01]], grad_fn=<AddmmBackward0>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Testing the model\n",
    "model.forward(training_tensor)\n",
    "\n",
    "# # Code to test out the encoder & decoder\n",
    "# mu, log_var = encoder.forward(training_tensor)\n",
    "# print(\"mu: \", mu.shape)\n",
    "# print(\"log_var: \", log_var.shape)\n",
    "\n",
    "# reconstructed_img = decoder.forward(mu, log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed_img, input_img, mu, log_var, kld_weight=2):\n",
    "    img_loss = F.mse_loss(reconstructed_img, input_img)\n",
    "    # article on calculating kl divergence between 2 gaussians:\n",
    "    # https://medium.com/@outerrencedl/variational-autoencoder-and-a-bit-kl-divergence-with-pytorch-ce04fd55d0d7\n",
    "    # kld_loss = torch.mean(\n",
    "    #     torch.sum(-log_var + (log_var.exp() ** 2 + mu**2) / 2 - 1 / 2)\n",
    "        \n",
    "    # )\n",
    "    kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp()))\n",
    "    kld_loss *= kld_weight\n",
    "\n",
    "    return img_loss + kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(model, loss_func, x, y, xvalid, yvalid, lr=0.1, steps=5000, batch_size=32):\n",
    "    # only really need x or y, they are the same thing\n",
    "    optimizer = optim.AdamW(model.parameters(), lr)\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    train_dataset = TensorDataset(x, y)\n",
    "    train_batches = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    valid_dataset = TensorDataset(xvalid, yvalid)\n",
    "    valid_batches = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for _ in tqdm.trange(steps):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_batch, label_batch in train_batches:\n",
    "            reconstructed_img, mu, log_var = model(input_batch)\n",
    "            # y is the original image I think? yeah\n",
    "            loss = loss_func(reconstructed_img, label_batch, mu, log_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "        mean_loss = total_loss/len(train_batches)\n",
    "        losses.append(mean_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_valid_loss = 0\n",
    "        for input_valid_batch, label_valid_batch in valid_batches:\n",
    "            reconstructed_img, mu, log_var = model(input_valid_batch)\n",
    "            valid_loss = loss_func(reconstructed_img, label_valid_batch, mu, log_var)\n",
    "            total_valid_loss += valid_loss.detach()\n",
    "        mean_valid_loss = total_valid_loss/len(valid_batches)\n",
    "        valid_losses.append(mean_valid_loss)\n",
    "\n",
    "    print(f\"Final training loss: {losses[-1]}\")\n",
    "\n",
    "    return losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]C:\\Users\\Joshua\\AppData\\Local\\Temp\\ipykernel_37472\\241648366.py:4: UserWarning: Using a target size (torch.Size([600, 3, 64, 64])) that is different to the input size (torch.Size([600, 3, 8, 8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  img_loss = F.mse_loss(reconstructed_img, input_img)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n",
      "reconstructed_img:  torch.Size([600, 3, 64, 64])\n",
      "input_img:  torch.Size([600, 3, 64, 64])\n",
      "encoded.shape: torch.Size([151, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([151, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5000 [00:06<9:10:45,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed_img:  torch.Size([151, 3, 64, 64])\n",
      "input_img:  torch.Size([151, 3, 64, 64])\n",
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n",
      "reconstructed_img:  torch.Size([600, 3, 64, 64])\n",
      "input_img:  torch.Size([600, 3, 64, 64])\n",
      "encoded.shape: torch.Size([151, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([151, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/5000 [00:12<8:39:02,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed_img:  torch.Size([151, 3, 64, 64])\n",
      "input_img:  torch.Size([151, 3, 64, 64])\n",
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n",
      "reconstructed_img:  torch.Size([600, 3, 64, 64])\n",
      "input_img:  torch.Size([600, 3, 64, 64])\n",
      "encoded.shape: torch.Size([151, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([151, 1152])\n",
      "reconstructed_img:  torch.Size([151, 3, 64, 64])\n",
      "input_img:  torch.Size([151, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/5000 [00:15<6:43:33,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n",
      "reconstructed_img:  torch.Size([600, 3, 64, 64])\n",
      "input_img:  torch.Size([600, 3, 64, 64])\n",
      "encoded.shape: torch.Size([151, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([151, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5000 [00:18<5:42:26,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed_img:  torch.Size([151, 3, 64, 64])\n",
      "input_img:  torch.Size([151, 3, 64, 64])\n",
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5000 [00:20<6:57:08,  5.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_tensor\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(model, loss_func, x, y, xvalid, yvalid, lr, steps)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtrange(steps):\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 9\u001b[0m     reconstructed_img, mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# y is the original image I think?\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(reconstructed_img, y, mu, log_var)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\morri\\OneDrive\\Documents\\GitHub\\vae-project\\vae.py:17\u001b[0m, in \u001b[0;36mBasic_VAE.forward\u001b[1;34m(self, input_img)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_img):\n\u001b[0;32m     16\u001b[0m     mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(input_img)\n\u001b[1;32m---> 17\u001b[0m     reconstructed_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [reconstructed_img, mu, log_var]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\morri\\OneDrive\\Documents\\GitHub\\vae-project\\decoder.py:112\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, mu, log_var)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mu, log_var):\n\u001b[0;32m    111\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, log_var)\n\u001b[1;32m--> 112\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\morri\\OneDrive\\Documents\\GitHub\\vae-project\\decoder.py:97\u001b[0m, in \u001b[0;36mDecoder.decode\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     91\u001b[0m res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# this -1 is for the batchsize, so the new result is batch size x depth x H x W\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dims[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim_mult \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim_mult \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m     96\u001b[0m )\n\u001b[1;32m---> 97\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, valid_losses = gradient_descent(\n",
    "    model, loss_function, training_tensor, training_tensor, cv_tensor, cv_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we've trained, we use the decoder to generate new images\n",
    "# we assume a normal distribution over our latent space\n",
    "# so we sample from that distribution and feed it into the decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import os\n",
    "import matplotlib.image as img\n",
    "import tqdm\n",
    "from vae import Basic_VAE\n",
    "from encoder import Encoder\n",
    "from decoder import Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "cats = []\n",
    "directory = \"data/cats\"\n",
    "count = 0\n",
    "for catpic in os.listdir(directory):\n",
    "    if count < 1000:\n",
    "        # read from image and convert to tensor\n",
    "        im = torch.tensor(img.imread(os.path.join(directory, catpic))).float()\n",
    "        # permute to (channels, height, width) for conv2d layer\n",
    "        im = torch.permute(im, (2, 0, 1))\n",
    "        # normalize to range between -1 and 1\n",
    "        im = im / 128 - 1\n",
    "        cats.append(im)\n",
    "        count += 1\n",
    "    else:\n",
    "        break\n",
    "cats = torch.stack(cats)\n",
    "print(cats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test/Training/Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 3, 64, 64])\n",
      "torch.Size([151, 3, 64, 64])\n",
      "torch.Size([249, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Split the data into what we use for testing and not testing\n",
    "training_test_split = 0.75\n",
    "training_test_cutoff = int(cats.shape[0] * training_test_split + 1)\n",
    "random_perm = torch.randperm(cats.shape[0])\n",
    "not_test_tensor = cats[:training_test_cutoff]\n",
    "testing_tensor = cats[training_test_cutoff:]\n",
    "# Split the data into what we use for training and cross validation\n",
    "training_cv_split = 0.8\n",
    "training_cv_cutoff = int(not_test_tensor.shape[0] * training_cv_split)\n",
    "training_tensor = not_test_tensor[:training_cv_cutoff]\n",
    "cv_tensor = not_test_tensor[training_cv_cutoff:]\n",
    "print(training_tensor.shape)\n",
    "print(cv_tensor.shape)\n",
    "print(testing_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose Hyperparameters and Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128, 64, 32, 16]\n"
     ]
    }
   ],
   "source": [
    "from vae import Basic_VAE\n",
    "\n",
    "hidden_dims = [16, 32, 64, 128]\n",
    "latent_dim = 64\n",
    "in_dim = 3\n",
    "model = Basic_VAE(in_dim, hidden_dims, latent_dim)\n",
    "# encoder = Encoder(in_dim, hidden_dims, latent_dim) for testing\n",
    "# decoder = Decoder(latent_dim, hidden_dims) for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[[[-1.5987e-01,  3.4740e-01,  1.1799e-01,  ...,  3.5794e-01,\n",
       "            -1.3262e-01,  2.5225e-01],\n",
       "           [ 5.2806e-01,  9.0487e-01,  4.3923e-02,  ...,  1.9971e-01,\n",
       "             1.6595e-01, -1.8641e-03],\n",
       "           [ 2.3152e-01,  6.6951e-01, -6.6842e-01,  ...,  9.9181e-02,\n",
       "            -3.0802e-01, -6.8947e-02],\n",
       "           ...,\n",
       "           [-1.8711e-01, -2.2090e-01,  1.9113e-01,  ...,  4.0890e-02,\n",
       "             1.7868e-01, -3.3205e-02],\n",
       "           [ 7.4919e-02, -1.2330e-01, -7.1301e-01,  ...,  1.1409e-01,\n",
       "             2.3821e-01,  1.0046e-01],\n",
       "           [-1.1839e-01, -1.1355e-01, -2.9124e-01,  ..., -6.8516e-02,\n",
       "            -1.6792e-02,  6.4432e-02]],\n",
       " \n",
       "          [[-1.1686e-01, -6.5236e-02,  8.2952e-01,  ...,  9.0968e-02,\n",
       "            -2.6019e-01, -2.2322e-01],\n",
       "           [ 2.2235e-01,  5.2316e-01, -7.5258e-02,  ..., -5.8584e-01,\n",
       "            -7.6346e-02,  2.2641e-01],\n",
       "           [ 6.9051e-01, -7.8847e-01,  9.0353e-02,  ...,  4.6588e-01,\n",
       "             6.1398e-02, -3.0085e-01],\n",
       "           ...,\n",
       "           [-1.3990e-01, -2.4265e-01, -4.5723e-01,  ..., -4.9643e-02,\n",
       "            -2.5549e-01, -2.7073e-02],\n",
       "           [-1.5610e-01, -1.3402e-01,  1.5473e-01,  ..., -1.6206e-01,\n",
       "            -1.2278e-01,  4.8185e-02],\n",
       "           [-1.6618e-01, -5.3566e-02, -3.3445e-01,  ..., -1.4085e-02,\n",
       "            -3.5764e-02, -3.4504e-02]],\n",
       " \n",
       "          [[-7.4861e-02, -8.9754e-02,  1.6699e-01,  ..., -3.8115e-02,\n",
       "            -1.3114e-02, -1.5017e-01],\n",
       "           [-6.0097e-02,  4.2732e-01, -1.2202e-01,  ..., -2.4277e-01,\n",
       "            -3.6907e-01, -3.1270e-01],\n",
       "           [-4.6668e-01,  3.3366e-01,  9.0102e-01,  ..., -2.8140e-01,\n",
       "            -4.9901e-01, -6.5975e-01],\n",
       "           ...,\n",
       "           [-1.4411e-01,  6.4724e-01, -2.7746e-01,  ..., -1.3693e-01,\n",
       "             6.2409e-02,  6.3191e-02],\n",
       "           [-1.2409e-02,  5.8473e-01, -5.0355e-01,  ..., -1.4281e-01,\n",
       "            -2.3882e-01,  1.6661e-02],\n",
       "           [-4.7314e-02, -7.8831e-02, -6.4692e-01,  ..., -4.6497e-02,\n",
       "            -1.2709e-01, -7.9326e-02]]],\n",
       " \n",
       " \n",
       "         [[[-1.2339e-01,  5.9136e-01,  2.4365e-01,  ..., -1.1275e-01,\n",
       "            -4.3865e-01,  5.1803e-02],\n",
       "           [ 6.6042e-01,  2.5507e-01,  7.6389e-01,  ..., -7.7362e-01,\n",
       "            -6.6364e-01, -1.6773e-01],\n",
       "           [-4.3035e-01, -3.5846e-01,  2.6448e-01,  ...,  7.5307e-01,\n",
       "            -5.1530e-01,  3.7454e-02],\n",
       "           ...,\n",
       "           [ 1.2806e-01, -1.9506e-01, -3.6404e-01,  ..., -5.7028e-03,\n",
       "             4.0902e-03,  1.4458e-01],\n",
       "           [ 4.2080e-01,  1.8444e-01, -7.8011e-01,  ...,  5.6374e-02,\n",
       "             1.6470e-01,  7.3995e-02],\n",
       "           [-8.6238e-02,  3.8467e-01,  1.6523e-01,  ...,  1.5077e-01,\n",
       "             2.1780e-01, -7.9112e-02]],\n",
       " \n",
       "          [[ 9.0074e-03,  4.4166e-01, -1.7216e-01,  ...,  3.1341e-01,\n",
       "            -1.7362e-02, -6.3746e-02],\n",
       "           [-6.2741e-02,  2.6550e-01, -3.6116e-02,  ..., -2.6750e-01,\n",
       "            -1.2374e-01, -4.4569e-01],\n",
       "           [ 4.1935e-01, -7.4783e-01,  8.3715e-02,  ..., -1.2030e-01,\n",
       "             5.8699e-01, -1.7562e-03],\n",
       "           ...,\n",
       "           [-2.2301e-01, -2.3458e-01, -5.7183e-01,  ..., -6.7102e-03,\n",
       "             2.0426e-01, -5.0851e-02],\n",
       "           [-9.6747e-02, -2.8081e-01,  3.0707e-01,  ..., -6.6732e-02,\n",
       "            -1.2027e-01, -1.4287e-01],\n",
       "           [-7.7068e-02, -3.3540e-02, -4.1986e-01,  ...,  2.0499e-01,\n",
       "            -4.2655e-02,  1.2661e-01]],\n",
       " \n",
       "          [[ 1.1503e-01, -2.4649e-01, -5.4702e-01,  ...,  2.8243e-01,\n",
       "            -2.3942e-01, -2.2733e-01],\n",
       "           [ 4.9189e-01,  6.5311e-01, -6.3345e-01,  ...,  3.1458e-01,\n",
       "            -4.1685e-02, -2.1334e-01],\n",
       "           [-5.2971e-01, -1.8230e-01,  5.0231e-02,  ...,  3.1636e-01,\n",
       "            -6.7195e-03, -1.6931e-01],\n",
       "           ...,\n",
       "           [-3.6629e-01,  7.3256e-01,  4.9438e-01,  ..., -1.7285e-01,\n",
       "             3.8035e-02,  1.6264e-01],\n",
       "           [ 2.4448e-01,  4.1240e-01, -1.3776e-01,  ..., -7.1078e-02,\n",
       "            -1.0190e-02, -6.4725e-02],\n",
       "           [ 3.9739e-02, -2.1545e-01, -5.4599e-01,  ...,  1.6394e-02,\n",
       "             1.3986e-01, -2.8686e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.6738e-01,  1.1014e-01,  2.5922e-01,  ..., -2.1101e-01,\n",
       "             1.4153e-02, -7.9042e-03],\n",
       "           [ 2.3447e-01,  6.3942e-01, -6.7957e-02,  ...,  4.4109e-01,\n",
       "             9.2564e-04, -3.5282e-01],\n",
       "           [ 5.6494e-01, -9.0063e-02, -1.2988e-01,  ..., -1.3161e-01,\n",
       "            -1.9337e-01,  1.8472e-01],\n",
       "           ...,\n",
       "           [-1.0626e-01,  3.8075e-02, -5.2791e-01,  ..., -3.0313e-03,\n",
       "             4.5102e-03,  6.9781e-02],\n",
       "           [-1.0691e-01,  1.6392e-02, -6.4630e-01,  ...,  1.0258e-01,\n",
       "            -6.4032e-02, -1.6988e-01],\n",
       "           [-1.9464e-01,  7.1097e-02,  2.7538e-01,  ...,  2.5240e-01,\n",
       "             1.6410e-01,  7.0796e-02]],\n",
       " \n",
       "          [[-1.0807e-01, -2.8552e-01,  4.2127e-01,  ..., -2.1757e-01,\n",
       "             1.7436e-01, -1.4742e-01],\n",
       "           [-8.9823e-02,  2.7167e-01, -5.2219e-01,  ..., -2.0632e-01,\n",
       "            -4.8952e-02, -1.5893e-01],\n",
       "           [-6.4371e-02, -5.9436e-01, -1.6476e-02,  ...,  6.4435e-02,\n",
       "            -2.2136e-01,  3.9847e-01],\n",
       "           ...,\n",
       "           [ 4.2534e-02, -2.5278e-01, -1.6503e-01,  ..., -1.9255e-01,\n",
       "            -1.5035e-01, -1.7142e-01],\n",
       "           [-6.1252e-01, -1.2423e-01,  4.0260e-01,  ...,  6.4143e-02,\n",
       "            -3.3142e-01,  2.7324e-02],\n",
       "           [-2.1843e-02,  7.2455e-02,  1.4973e-01,  ..., -1.5360e-01,\n",
       "            -8.2523e-02, -1.6223e-02]],\n",
       " \n",
       "          [[-1.3674e-01, -6.1359e-02,  2.3263e-01,  ...,  6.0912e-02,\n",
       "            -2.7986e-02,  2.0238e-02],\n",
       "           [ 2.8515e-01,  5.3527e-01,  2.8038e-02,  ..., -8.9450e-02,\n",
       "            -4.0918e-01, -1.3178e-01],\n",
       "           [-9.2041e-03, -1.8388e-01,  6.8129e-02,  ..., -4.8210e-01,\n",
       "            -2.3818e-01,  3.5994e-01],\n",
       "           ...,\n",
       "           [-2.9504e-01,  2.4310e-01, -9.0097e-01,  ...,  6.0317e-02,\n",
       "            -3.8575e-02,  1.2629e-01],\n",
       "           [-2.5634e-01, -3.9359e-01, -5.0468e-01,  ...,  1.2274e-01,\n",
       "            -2.1634e-01,  1.5999e-01],\n",
       "           [-4.2918e-01,  4.0272e-02, -6.5420e-01,  ..., -1.7462e-01,\n",
       "             6.5226e-02,  2.7014e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-8.7157e-02, -1.6158e-01, -5.3062e-02,  ..., -1.4009e-01,\n",
       "             1.4656e-02,  6.8884e-02],\n",
       "           [-1.8563e-01,  3.7199e-01,  4.6923e-01,  ..., -5.4906e-01,\n",
       "            -4.2540e-01, -1.5391e-01],\n",
       "           [-7.3982e-01, -4.3779e-02, -1.1616e-02,  ..., -8.2533e-02,\n",
       "            -8.5746e-02, -3.4647e-01],\n",
       "           ...,\n",
       "           [-9.9844e-02, -1.7876e-01,  2.6336e-01,  ...,  3.3465e-01,\n",
       "             1.2600e-01, -6.7264e-03],\n",
       "           [ 5.3973e-02, -2.4282e-01, -2.2359e-01,  ...,  1.6878e-01,\n",
       "             3.7336e-01,  2.1203e-01],\n",
       "           [-5.1039e-02,  7.6518e-03, -2.7170e-01,  ..., -3.4009e-01,\n",
       "             6.6001e-02, -1.7372e-01]],\n",
       " \n",
       "          [[-2.1296e-02, -1.8194e-01, -1.6098e-01,  ..., -2.3381e-01,\n",
       "            -3.9529e-01, -1.9487e-02],\n",
       "           [-3.2247e-01, -3.7490e-02,  1.8561e-01,  ...,  4.9006e-02,\n",
       "             5.6203e-02, -1.2934e-01],\n",
       "           [-3.5837e-02, -2.6164e-02,  4.4033e-01,  ..., -5.4305e-01,\n",
       "             7.3751e-01, -5.9060e-01],\n",
       "           ...,\n",
       "           [-1.8044e-01, -5.7937e-02, -2.7681e-01,  ...,  1.5074e-01,\n",
       "             1.6522e-01,  8.5210e-02],\n",
       "           [ 1.7709e-01, -2.3276e-01,  9.1038e-02,  ...,  2.6413e-02,\n",
       "            -1.4691e-01, -1.2850e-03],\n",
       "           [-5.0104e-02, -9.3092e-02, -2.4340e-01,  ...,  3.2660e-01,\n",
       "             2.6820e-02,  2.0284e-01]],\n",
       " \n",
       "          [[-2.1968e-01, -2.4031e-02, -3.2624e-01,  ..., -3.3803e-01,\n",
       "            -1.2108e-01, -8.8352e-02],\n",
       "           [-3.9877e-01, -4.1917e-01, -6.3043e-01,  ...,  1.0484e-01,\n",
       "             1.6698e-01,  4.0620e-01],\n",
       "           [-3.2719e-01, -5.9822e-01, -7.9940e-01,  ...,  5.7635e-01,\n",
       "             1.1245e-01, -1.8489e-01],\n",
       "           ...,\n",
       "           [-2.2642e-01,  3.7850e-01,  6.3318e-01,  ..., -4.8156e-01,\n",
       "            -1.3805e-01,  1.2684e-02],\n",
       "           [ 1.5845e-01,  8.5096e-02, -1.8981e-01,  ..., -5.2019e-01,\n",
       "            -4.7867e-02, -7.3584e-02],\n",
       "           [-5.8527e-02, -2.1068e-01, -3.5272e-01,  ...,  2.2136e-01,\n",
       "            -8.9915e-03,  2.5953e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 2.0310e-01, -1.7031e-01, -4.1471e-01,  ...,  8.0953e-03,\n",
       "             1.2359e-01,  3.0859e-01],\n",
       "           [ 1.4223e-01, -2.1045e-01,  5.8092e-01,  ...,  6.3355e-03,\n",
       "             2.6380e-01, -2.4367e-01],\n",
       "           [ 5.3129e-02, -5.6041e-01,  1.6622e-01,  ..., -2.8518e-01,\n",
       "             5.0954e-01, -2.3078e-01],\n",
       "           ...,\n",
       "           [ 2.6985e-01, -4.6173e-02, -7.3894e-01,  ...,  3.1366e-01,\n",
       "             1.3882e-01,  2.6292e-01],\n",
       "           [ 6.2214e-01,  9.5054e-01, -3.1253e-01,  ..., -9.7152e-02,\n",
       "            -1.5545e-01, -2.3904e-01],\n",
       "           [-4.3284e-02,  2.4280e-01,  8.1071e-01,  ..., -7.6833e-02,\n",
       "             7.1604e-02, -8.9149e-02]],\n",
       " \n",
       "          [[-9.8114e-02,  1.9675e-01, -2.8511e-01,  ..., -2.7444e-01,\n",
       "            -2.8233e-01, -1.9769e-01],\n",
       "           [-1.8269e-01, -2.3681e-01, -3.5996e-01,  ..., -9.0455e-02,\n",
       "            -2.8202e-03, -1.4816e-01],\n",
       "           [ 1.0424e-01,  6.3672e-01, -7.6337e-01,  ..., -3.4643e-01,\n",
       "             8.9518e-03, -1.1513e-01],\n",
       "           ...,\n",
       "           [-6.0782e-01, -2.2997e-01,  2.9150e-01,  ..., -3.1556e-01,\n",
       "             5.4402e-02, -1.8861e-01],\n",
       "           [-5.1383e-01, -7.1310e-01, -3.6329e-01,  ..., -1.5532e-01,\n",
       "            -5.5531e-02,  2.7091e-02],\n",
       "           [-4.0112e-01, -4.4583e-01, -3.7420e-01,  ...,  4.4406e-02,\n",
       "            -2.0567e-02,  2.5562e-02]],\n",
       " \n",
       "          [[-1.9627e-01,  1.3868e-01,  6.8930e-01,  ..., -1.7293e-01,\n",
       "            -2.5996e-01, -2.3527e-01],\n",
       "           [ 1.1389e-01, -1.4092e-01, -1.3621e-02,  ..., -2.0663e-01,\n",
       "            -2.1040e-01,  4.1843e-02],\n",
       "           [-1.7238e-01, -9.0916e-01, -4.5480e-02,  ...,  3.7338e-01,\n",
       "            -3.8024e-01, -2.9980e-01],\n",
       "           ...,\n",
       "           [-6.8534e-01,  3.1737e-01,  8.4179e-01,  ...,  1.1790e-01,\n",
       "             6.0261e-02, -1.3165e-01],\n",
       "           [-4.7640e-01,  1.7324e-01,  3.1214e-01,  ..., -2.0709e-02,\n",
       "             5.9625e-02, -1.6877e-03],\n",
       "           [-1.0151e-01, -1.2196e-01,  8.6131e-01,  ...,  1.7163e-01,\n",
       "            -3.4749e-02, -2.1579e-01]]],\n",
       " \n",
       " \n",
       "         [[[-5.5126e-01, -1.9397e-01, -4.7081e-01,  ...,  2.0079e-01,\n",
       "             1.0365e-01,  2.3317e-01],\n",
       "           [-3.9825e-01, -8.3355e-01,  6.1745e-01,  ...,  4.1034e-01,\n",
       "             3.8209e-01,  5.2211e-01],\n",
       "           [-3.7195e-01, -3.9670e-01, -7.1199e-01,  ..., -2.2381e-01,\n",
       "            -2.4459e-01, -1.3854e-01],\n",
       "           ...,\n",
       "           [ 3.3250e-01,  2.5177e-01,  6.3694e-01,  ..., -2.9916e-01,\n",
       "            -2.8609e-02,  1.4644e-01],\n",
       "           [-3.9105e-01, -5.9987e-01,  1.6100e-01,  ..., -3.2644e-01,\n",
       "             5.0310e-02,  1.0786e-01],\n",
       "           [ 1.7678e-01, -4.1226e-01, -5.6890e-01,  ..., -2.7281e-01,\n",
       "            -2.1945e-01,  3.1000e-02]],\n",
       " \n",
       "          [[-6.7300e-02,  5.9954e-01, -9.7707e-02,  ..., -5.1056e-02,\n",
       "             6.6141e-02, -2.0942e-01],\n",
       "           [ 1.6378e-01, -4.9202e-01, -5.0705e-01,  ..., -2.7270e-01,\n",
       "            -3.4749e-02, -1.4546e-02],\n",
       "           [ 2.9958e-01, -3.6394e-01, -3.0803e-01,  ...,  2.4429e-01,\n",
       "            -5.1924e-01, -2.3412e-01],\n",
       "           ...,\n",
       "           [ 1.6658e-01,  2.8155e-01, -1.7949e-01,  ..., -1.4384e-01,\n",
       "            -2.3599e-01, -1.3387e-01],\n",
       "           [ 4.5522e-01,  5.7577e-01,  9.3540e-02,  ..., -1.6903e-01,\n",
       "            -5.7583e-02, -2.2500e-01],\n",
       "           [ 3.1931e-01,  2.3217e-01,  1.2443e-01,  ..., -2.5154e-01,\n",
       "            -1.7854e-01, -2.6039e-01]],\n",
       " \n",
       "          [[ 8.0421e-02,  5.5961e-01, -1.1408e-01,  ..., -2.4919e-01,\n",
       "            -1.3348e-01,  1.3053e-02],\n",
       "           [-4.0853e-01, -2.9641e-01, -4.9966e-01,  ..., -4.9298e-01,\n",
       "            -5.0572e-01, -3.4615e-01],\n",
       "           [ 1.2016e-01,  4.9547e-01, -8.3498e-01,  ...,  2.3330e-01,\n",
       "            -2.0453e-01, -2.4857e-01],\n",
       "           ...,\n",
       "           [ 5.2699e-01, -7.2663e-01, -6.6812e-01,  ..., -3.3831e-01,\n",
       "             8.1924e-03, -3.3603e-01],\n",
       "           [ 2.7983e-01, -4.8443e-01, -3.1917e-01,  ...,  8.6201e-03,\n",
       "            -1.9221e-01, -1.2791e-01],\n",
       "           [-2.9691e-02, -3.1274e-01, -6.2568e-01,  ..., -1.8914e-01,\n",
       "            -2.3100e-01, -2.1602e-01]]]], grad_fn=<TanhBackward0>),\n",
       " tensor([[-0.1854,  0.0771, -0.2865,  ...,  0.1005,  0.3562, -0.2371],\n",
       "         [-0.1206, -0.3105,  0.1369,  ...,  0.0890,  0.1330,  0.0045],\n",
       "         [-0.8505,  0.2732,  0.1786,  ...,  0.5413, -0.5167,  0.0184],\n",
       "         ...,\n",
       "         [-0.0640,  0.0788,  0.1524,  ...,  0.2134,  0.2788, -0.2525],\n",
       "         [ 0.1525,  0.2049,  0.1275,  ...,  0.3130, -0.0975,  0.2093],\n",
       "         [-0.1027,  0.0303,  0.4613,  ...,  0.1083,  0.2936,  0.0213]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2752, -0.1496, -0.0762,  ...,  0.0504, -0.2395, -0.3446],\n",
       "         [-0.0130, -0.0960, -0.2050,  ..., -0.3589, -0.1322, -0.1775],\n",
       "         [ 0.6643, -0.1271, -0.0296,  ...,  0.6396, -0.7281, -0.5986],\n",
       "         ...,\n",
       "         [-0.0898,  0.1272, -0.1218,  ..., -0.3258, -0.1478, -0.1801],\n",
       "         [ 0.0068,  0.0158, -0.0273,  ..., -0.4878, -0.1743,  0.0802],\n",
       "         [-0.3828, -0.5329, -0.1381,  ...,  0.1045, -0.0249,  0.1512]],\n",
       "        grad_fn=<AddmmBackward0>)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Testing the model\n",
    "model.forward(training_tensor)\n",
    "\n",
    "# # Code to test out the encoder & decoder\n",
    "# mu, log_var = encoder.forward(training_tensor)\n",
    "# print(\"mu: \", mu.shape)\n",
    "# print(\"log_var: \", log_var.shape)\n",
    "\n",
    "# reconstructed_img = decoder.forward(mu, log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed_img, input_img, mu, log_var, kld_weight=2):\n",
    "    print(\"reconstructed_img: \", reconstructed_img.shape)\n",
    "    print(\"input_img: \", input_img.shape)\n",
    "    img_loss = F.mse_loss(reconstructed_img, input_img)\n",
    "    # article on calculating kl divergence between 2 gaussians:\n",
    "    # https://medium.com/@outerrencedl/variational-autoencoder-and-a-bit-kl-divergence-with-pytorch-ce04fd55d0d7\n",
    "    kld_loss = torch.mean(\n",
    "        torch.sum(-log_var + (log_var.exp() ** 2 + mu**2) / 2 - 1 / 2)\n",
    "    )\n",
    "    kld_loss *= kld_weight\n",
    "\n",
    "    return img_loss + kld_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(model, loss_func, x, y, xvalid, yvalid, lr=0.1, steps=5000):\n",
    "    # only really need x or y, they are the same thing\n",
    "    optimizer = optim.AdamW(model.parameters(), lr)\n",
    "\n",
    "    losses = []\n",
    "    valid_losses = []\n",
    "    for _ in tqdm.trange(steps):\n",
    "        model.train()\n",
    "        reconstructed_img, mu, log_var = model(x)\n",
    "        # y is the original image I think?\n",
    "        loss = loss_func(reconstructed_img, y, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        reconstructed_img, mu, log_var = model(xvalid)\n",
    "        valid_loss = loss_func(reconstructed_img, yvalid, mu, log_var)\n",
    "        losses.append(loss.detach().numpy())\n",
    "        valid_losses.append(valid_loss.detach().numpy())\n",
    "\n",
    "    print(f\"Final training loss: {losses[-1]}\")\n",
    "\n",
    "    return losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n",
      "reconstructed_img:  torch.Size([600, 3, 64, 64])\n",
      "input_img:  torch.Size([600, 3, 64, 64])\n",
      "encoded.shape: torch.Size([151, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([151, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5000 [00:06<9:10:45,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed_img:  torch.Size([151, 3, 64, 64])\n",
      "input_img:  torch.Size([151, 3, 64, 64])\n",
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n",
      "reconstructed_img:  torch.Size([600, 3, 64, 64])\n",
      "input_img:  torch.Size([600, 3, 64, 64])\n",
      "encoded.shape: torch.Size([151, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([151, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/5000 [00:12<8:39:02,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed_img:  torch.Size([151, 3, 64, 64])\n",
      "input_img:  torch.Size([151, 3, 64, 64])\n",
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n",
      "reconstructed_img:  torch.Size([600, 3, 64, 64])\n",
      "input_img:  torch.Size([600, 3, 64, 64])\n",
      "encoded.shape: torch.Size([151, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([151, 1152])\n",
      "reconstructed_img:  torch.Size([151, 3, 64, 64])\n",
      "input_img:  torch.Size([151, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/5000 [00:15<6:43:33,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n",
      "reconstructed_img:  torch.Size([600, 3, 64, 64])\n",
      "input_img:  torch.Size([600, 3, 64, 64])\n",
      "encoded.shape: torch.Size([151, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([151, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5000 [00:18<5:42:26,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstructed_img:  torch.Size([151, 3, 64, 64])\n",
      "input_img:  torch.Size([151, 3, 64, 64])\n",
      "encoded.shape: torch.Size([600, 128, 3, 3])\n",
      "encoded.shape after flatten: torch.Size([600, 1152])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/5000 [00:20<6:57:08,  5.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m losses, valid_losses \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv_tensor\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(model, loss_func, x, y, xvalid, yvalid, lr, steps)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtrange(steps):\n\u001b[0;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 9\u001b[0m     reconstructed_img, mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# y is the original image I think?\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(reconstructed_img, y, mu, log_var)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\morri\\OneDrive\\Documents\\GitHub\\vae-project\\vae.py:17\u001b[0m, in \u001b[0;36mBasic_VAE.forward\u001b[1;34m(self, input_img)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_img):\n\u001b[0;32m     16\u001b[0m     mu, log_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(input_img)\n\u001b[1;32m---> 17\u001b[0m     reconstructed_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [reconstructed_img, mu, log_var]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\morri\\OneDrive\\Documents\\GitHub\\vae-project\\decoder.py:112\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, mu, log_var)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mu, log_var):\n\u001b[0;32m    111\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, log_var)\n\u001b[1;32m--> 112\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\morri\\OneDrive\\Documents\\GitHub\\vae-project\\decoder.py:97\u001b[0m, in \u001b[0;36mDecoder.decode\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     91\u001b[0m res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# this -1 is for the batchsize, so the new result is batch size x depth x H x W\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dims[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim_mult \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim_mult \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m0.5\u001b[39m)),\n\u001b[0;32m     96\u001b[0m )\n\u001b[1;32m---> 97\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, valid_losses = gradient_descent(\n",
    "    model, loss_function, training_tensor, training_tensor, cv_tensor, cv_tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we've trained, we use the decoder to generate new images\n",
    "# we assume a normal distribution over our latent space\n",
    "# so we sample from that distribution and feed it into the decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
